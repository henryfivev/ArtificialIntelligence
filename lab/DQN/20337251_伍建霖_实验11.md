# 中山大学计算机学院

# **人工智能本科生实验报告**

 

课程名称：Artificial Intelligence

| 教学班级 | 网安软工合班 | 专业（方向） | 网络空间安全 |
| -------- | ------------ | ------------ | ------------ |
| 学号     | 20337251     | 姓名         | 伍建霖       |

## 一、实验题目

<img src="D:\CodeField\TP\image-20220711195816432.png" alt="image-20220711195816432" style="zoom:40%;" />

## 二、实验内容

### 1.算法原理

​	DQN与QLearning的区别：由于动作和状态是连续的，无法用一个表格来存储，故用一个人工神经网络来代替Q表

​	DQN是一种off-policy的算法，同时新增了replay buffer和target net的功能

​	replay buffer，将过去的数据存放在一个列表中，每隔一个batch学习一次

​	target net，和原来的网络一模一样，将原来的网络作为评估网络，target net作为目标网络。在学习过程中，使用目标网络进行自益得到回报的评估值，作为学习目标。在更新过程中，只更新评估网络的权重，而不更新目标网络的权重。这样，更新权重时针对的目标不会在每次迭代都发生变化，是一个固定的目标。在更新一定次数后，再将评估网络的权重复制给目标网络，进而进行下一批更新，这样目标网络也能得到更新。由于在目标网络没有变化的一段时间内回报的估计是相对固定的，因此目标网络的引入增加了学习的稳定性。

### 2. 伪代码

<img src="D:\CodeField\TP\image-20220711221014079.png" alt="image-20220711221014079" style="zoom:33%;" />

### 3.关键代码展示（带注释）

```python
class QNetwork(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim) -> None:
        super(QNetwork, self).__init__()
        self.layer1 = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim),
            torch.nn.PReLU()
        )

        self.layer2 = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim),
            torch.nn.PReLU()
        )

        self.layer3 = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim),
            torch.nn.PReLU()
        )

        self.final = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.final(x)
        return x
    
# learn函数在agentdqn类中
def learn(self, gamma):
        if len(self.replay_memory.memory) < BATCH_SIZE:
            return
        # 随机取batch_size个历史数据
        transitions = self.replay_memory.sample(BATCH_SIZE)
        # 把数据转成设置格式，方便索索引
        batch = Transition(*zip(*transitions))

        states = torch.cat(batch.state)
        actions = torch.cat(batch.action)
        rewards = torch.cat(batch.reward)
        next_states = torch.cat(batch.next_state)
        dones = torch.cat(batch.done)

        Q_expected = self.q_local(states).gather(1, actions)
        Q_targets_next = self.q_target(
            next_states).detach().max(1)[0]  # 下一步的期望
        Q_targets = rewards + (gamma * Q_targets_next * (1-dones))  # 相当于真值

        # self.q_local.train(mode=True)
        self.optim.zero_grad()
        loss = self.mse_loss(Q_expected, Q_targets.unsqueeze(1))
        loss.backward()
        self.optim.step()
        
def get_action(self, state, eps, check_eps=True):
        global steps_done
        # 产生随机数，用于和eps对比
        sample = random.random()
        # 如果随机数大于eps，就选择使用DQN网络输出的action，否则随机选择一个动作
        if check_eps == False or sample > eps:
            with torch.no_grad():
                return self.q_local(Variable(state).type(FloatTensor)).data.max(1)[1].view(1, 1)
        else:
            # 返回动作动作区间里面的随机的一个动作
            return torch.tensor([[random.randrange(self.n_actions)]], device=device)
```

### 4. 优化

将DQN网络架构定义成三层MLP网络，以获得更好的拟合能力

## 三、实验结果及分析

### 1. 实验结果展示示例（可图可表可文字，尽量可视化）

#### batch = 64， learning rate = 0.02

<img src="D:\CodeField\TP\image-20220711203950419.png" alt="image-20220711203950419" style="zoom:30%;" />

<img src="D:\CodeField\TP\image-20220711204018721.png" alt="image-20220711204018721" style="zoom:30%;" />

#### batch = 64，learning rate = 0.001

<img src="D:\CodeField\TP\image-20220711233905921.png" alt="image-20220711233905921" style="zoom:33%;" />

### 2.评测指标展示及分析（其它可分析运行时间等）

​	当batch_size为32时，平均得分收敛至150(lr=0.001)，或100(lr=0.01)

​	当batch_size为64时，平均得分收敛至175(lr=0.02)，或130(lr=0.01)，或在150至183之间来回波动(lr=0.001)

## 四、参考资料

PPT

[dqn/dqn.py at master · tokb23/dqn (github.com)](https://github.com/tokb23/dqn/blob/master/dqn.py)

[Deep-Q-Network-Breakout/agent_dqn.py at master · ShanHaoYu/Deep-Q-Network-Breakout (github.com)](https://github.com/ShanHaoYu/Deep-Q-Network-Breakout/blob/master/agent_dir/agent_dqn.py)

[【强化学习】Deep Q-Network (DQN) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/108286901)

[【动手学强化学习（1）DQN】CarPole DQN的实现 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/351428903)

[PyTorch实现DQN强化学习 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/117287339)

[【强化学习】Deep Q-Network (DQN) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/108286901)

