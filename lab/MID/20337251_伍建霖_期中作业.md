# 中山大学计算机学院

# **人工智能本科生实验报告**

 

课程名称：Artificial Intelligence

| 教学班级 | 网安软工合班 | 专业（方向） | 网络空间安全 |
| -------- | ------------ | ------------ | ------------ |
| 学号     | 20337251     | 姓名         | 伍建霖       |

## 一、实验题目

<img src="D:\CodeField\TP\image-20220502161653572.png" alt="image-20220502161653572" style="zoom:50%;" />

## 二、实验内容

### 1.算法原理

​	Alpha-Beta算法是在Min-Max方法基础上的一个改进。它维护一个搜索窗口：[α, β]。其中α当前对抗者能确保达到的最大的结点值，在进一步的搜索中，将竭力提高α这个下限。β表示在搜索进行到当前状态,在对手逼迫下，当前对抗者所达到的最小的博弈值。如果α >= β，那么就没有必要再搜索这个结点及其子结点了。

#### 通俗解释

​	A和B博弈，假设A和B都足够聪明，会选择当前利益最大化的步子。A为了最大利益，选择最大值，B为了使A最小利益，选择最小值

### 2. 伪代码

```python
def alphabeta(深度, a, b, 棋盘):
	if 达到最大深度:
        返回评估值
    遍历棋盘上的棋子:
        遍历该棋子可能的走法:
            走出该走法
            ret = alphabeta(深度+1, a, b, 棋盘)
            退回该走法
            if 轮到我走:
                if 该走法更优且是下一步的走法:
                    保存该棋子的位置和走法
                a = max(a, ret)
            else:
                b = min(b, ret)
            if a>=b:
                if 轮到我走:
                    return a
                else:
                    return b
    if 轮到我走:
        return a
    else:
        return b
```

### 3.关键代码展示（带注释）

```python
class MyAI(object):
    def __init__(self, computer_team):
        self.cnt = 0
        self.sum = 0
        self.old_pos = []
        self.new_pos = []
        self.history_pos1 = []
        self.history_pos2 = []
        self.team = computer_team
        if self.team == 'r':
            self.max_depth = 6
        else:
            self.max_depth = 5
        self.evaluate_class = Evaluate(self.team)

    def get_next_step(self, chessboard: ChessBoard):
        self.cnt += 1
        print("现在是第{}回合{}方走子".format(self.cnt, self.team))
        self.old_pos = []
        self.new_pos = []
        val = self.alpha_beta(1, -100000, 100000, chessboard)
        print("eva =", val)
        self.sum += val
        print("sum =", self.sum)
        cur_row,  cur_col  = self.old_pos
        next_row, next_col = self.new_pos
        self.judge_repeat()
        # self.enough_evaluate(self.sum)
        # self.low_evaluate(self.sum)
        self.store_history()
        return cur_row, cur_col, next_row, next_col

    def store_history(self):
        """
        保存上一步和当前的走法
        """
        if self.cnt == 1:
            self.history_pos1 = self.new_pos
        else:
            self.history_pos2 = self.history_pos1
            self.history_pos1 = self.new_pos

    def judge_repeat(self):
        """
        当cnt大于3时,history_pos1和history_pos2
        分别保存着上一步和上上步的走法
        """
        if self.history_pos2 == self.new_pos:
            self.max_depth += 1
            print("发现重复的情况")
            print("已将深度加大至", self.max_depth)
            print("下一回合请耐心等待")
        return 

    def capture_evaluate(self, new_row, new_col, chessboard:ChessBoard):
        if chessboard.chessboard_map[new_row][new_col] != None:
            # 有吃子的情况
            return -0.01 * self.evaluate_class.\
                get_single_chess_point(chessboard.chessboard_map[new_row][new_col])
        else:
            # 无吃子的情况
            return 0

    def enough_evaluate(self, sum):
        if sum > 1000:
            self.max_depth -= 0.2
            print("局面向好")
            print("已将深度减小至", self.max_depth)

    def low_evaluate(self, sum):
        if sum < -100:
            self.max_depth += 0.1
            print("局面不利")
            print("已将深度加大至", self.max_depth)
            print("下一回合请耐心等待")

    def alpha_beta(self, depth, a, b, chessboard:ChessBoard):
        if (depth >= self.max_depth):
            return self.evaluate_class.evaluate(chessboard)
        all_chess = chessboard.get_chess()
        # 遍历棋盘
        for chess in all_chess:
            if (depth%2 == 1 and chess.team == self.team) or \
                (depth%2 == 0 and chess.team != self.team):
                possible_pos = chessboard.get_put_down_position(chess)
                # 遍历该棋子可能的走法
                for next_row, next_col in possible_pos:
                    old_row, old_col = chess.row, chess.col
                    chess_in_new_pos = chessboard.chessboard_map[next_row][next_col]
                    # 移动棋子
                    chessboard.chessboard_map[next_row][next_col] = \
                        chessboard.chessboard_map[old_row][old_col]

                    chessboard.chessboard_map[next_row][next_col].\
                        update_position(next_row, next_col)

                    chessboard.chessboard_map[old_row][old_col] = None
                    # 进入下一层
                    ret = self.alpha_beta(depth+1, a, b, chessboard)
                    # 中盘再加入能否吃子的评估
                    # if self.cnt >= 10:
                    #     ret += self.capture_evaluate(next_row, next_col, chessboard)
                    # 摆回棋子
                    chessboard.chessboard_map[old_row][old_col] = \
                        chessboard.chessboard_map[next_row][next_col]
                    chessboard.chessboard_map[old_row][old_col].\
                        update_position(old_row, old_col)
                    chessboard.chessboard_map[next_row][next_col] = chess_in_new_pos
                    # 保存与剪枝
                    if (depth%2 == 1):
                        if (ret > a or self.old_pos == []) and depth == 1:
                            # 保存get_next_step的返回值
                            self.old_pos = [chess.row, chess.col]
                            self.new_pos = [next_row, next_col]
                        a = max(a, ret)
                    else:
                        b = min(b, ret)
                    if (a >= b):
                        if (depth%2 == 1):
                            return a
                        else:
                            return b
        if (depth%2 == 1):
            return a
        else:
            return b
```

### 4.创新点&优化（如果有）

#### 可用机制

1.发现重复走子时,通过加大深度来走出困境

---使用history_pos1和history_pos2分别记录上一步和上上步

4.走出困境后降低深度

---使用 sum=历次evaluate之和 来评估局面是否向好

---也可以使用更多的内存来记录历史棋子以判断是否走出困境,例如：

------使用history_pos来记录所有的new_pos,判断邻近的几步中是否有相同的走法

---还可以动态调整搜索深度来实现,但这样操作变多,浪费时间,例如:

------在getnextstep的开始和结束分别depth+=trouble和depth-=trouble,在困境中调整trouble的值

6.当评估值过低时,加大搜索深度

---和机制4属于类似操作



#### TODO

5.开局中盘不同评估值

---要么开局以动用的棋子数为评估值

------一个炮走两步vs两个炮走一步

---要么单纯的使用两套评估值

7.尝试不同的评估函数

---判断阵型,如二鬼拍门之类的

8.引入开局库

---使用网上的开局库来加快开局时的速度

9.动态子力

---开局，中盘，残局时使用不同的子力评估

10.残局开启吃子评估

---避免我因重复走子获胜的情况(减少利用规则获胜的情况)



#### 放弃的机制

2.中盘开启吃子评估

---深度为5时下不过chessai,深度为6时耗时过长

3.时间判断,避免加大深度后运行过长时间

---需要修改alphabeta函数,且意义不大,中途退出可能导致不利局面



考虑:

1.位置重要还是子力重要

---评估值的影响

---当前子力参考了网上的象棋子力表

2.马重要还是炮重要

---炮重要,当马的评估值大于炮时易失败

3.残局子力是否改变

## 三、实验结果及分析

main.py红方 main3.py黑方

黑屏不要着急，等myai算出下一步即可显示gui

测试请  不要点击任何位置  或  切到其他界面(滚轮滑动可以)

### 1. 实验结果展示示例（可图可表可文字，尽量可视化）

<img src="D:\CodeField\TP\image-20220429212725745.png" alt="image-20220429212725745" style="zoom:50%;" />

<img src="D:\CodeField\TP\image-20220501200939520.png" alt="image-20220501200939520" style="zoom:50%;" />

​	对战助教给的ai，作为红方或者黑方都获胜了，但作为黑方获胜时明明可以将死ai，却"陷入"了重复走子获胜的情况

### 2.评测指标展示及分析（其它可分析运行时间等）

​	两次获胜的时间都在20到30分钟之间，约60到80回合

### 3.对放弃中盘开启吃子评估的一点解释

​	对于吃子评估，我尝试过“开局开启”和“中盘开启”，开局就开启吃子评估会出现第一步就用炮打掉对方马的情况(中盘开启时也会有类似操作)，我认为出现这种“吃子>一切”的情况是因为自带的evaluate函数中就已经评估过这次“吃子”了，我的吃子评估机制“矫枉过正”了

​	同时我还尝试了深度为6时使用吃子评估，然而不知道为什么这导致了每一步都多使用几分钟的情况

## 四、参考资料

### 算法

[实现AI下井字棋的alpha-beta剪枝算法（python实现）_不基调的博客-CSDN博客_井字棋剪枝算法](https://blog.csdn.net/weixin_40804043/article/details/86704637)

### 策略

[计算机博弈 - 象棋百科全书 (xqbase.com)](https://www.xqbase.com/computer.htm)

