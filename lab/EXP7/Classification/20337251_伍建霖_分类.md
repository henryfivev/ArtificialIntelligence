# 中山大学计算机学院

# **人工智能本科生实验报告**

 

课程名称：Artificial Intelligence

| 教学班级 | 网安软工合班 | 专业（方向） | 网络空间安全 |
| -------- | ------------ | ------------ | ------------ |
| 学号     | 20337251     | 姓名         | 伍建霖       |

## 一、实验题目

<img src="D:\CodeField\TP\image-20220522192913658.png" alt="image-20220522192913658" style="zoom:50%;" />

## 二、实验内容

### 1.算法原理

预测值 ${y = g(𝑧) =}$ ${1\over 1+e^{-z}}$，${z =}$ ${w^Tx+b}$，${\dot{g} = g(z)(1-g(z))}$

损失函数 ${J(w) = }$ ${-1 \over m}$ $\sum_{i=1}^m({-ylog(\hat{y})-(1-y)log(1-\hat{y})})$

梯度下降 ${w = w - \alpha}$ ${{\partial J}\over{\partial w}}$

### 2. 伪代码

```python
def descent:
    设置学习率，迭代次数等参数
    while True:
        计算梯度方向
        更新参数
        计算损失值
        判断是否return
```

### 3.关键代码展示（带注释）

```python
def descent(data,theta,batchsize,stoptype,thresh,alpha):
    i=0  #迭代次数
    k=0  #batch
    X,y=shuffleData(data)  #洗牌
    grad=np.zeros_like(theta)  #梯度方向
    costs=[cost(X,y,theta)]  #计算代价函数的损失值
    
    while True:
        #1.计算梯度方向grad，batchsize始值选择样本的数量
        grad=gradient(X[k:k+batchsize],y[k:k+batchsize],theta) 
        #设置循环，每次迭代都选择新的一组[k,k+batchsize]的样本进行迭代
        k=k + batchsize
        #如果循环过程中k超过了样本总量，则让它回归至0，重新洗牌，再次循环
        if k >= n:
            k=0
            X,y=shuffleData(data)
        #2.根据梯度下降公式求解theta值
        theta=theta - alpha*grad
        #同时将每次迭代中代价函数的损失值计算出来
        costs.append(cost(X,y,theta)) 
        cost1.append(cost(X,y,theta))
        #3.选择下降停止策略：
        #循环计算次数i
        i=i+1  
        #如果选择根据迭代次数停止，则value=迭代次数
        if stoptype==STOP_ITER:
           value=i
        #如果根据代价损失来停止，则value=代价函数
        elif stoptype==STOP_COST:
           value=costs
        #若根据梯度方向来停止，则value=梯度下降方向
        elif  stoptype==STOP_GRAD:
           value=grad
        #直到所选择的value都满足阈值thresh，则停止循环
        if stopcriterion(stoptype,value,thresh):
            break
        #4.返回最后更新的theta值，迭代次数，损失值和梯度方向
    return theta,i-1,grad
```

### 4.优化

#### 扁平化

没有扁平化，那么error应该是一个(100，1)的二维数组，X[:, j]是一个一维数组，如果通过multiply对应相乘，那么得到的结果将会是一个100X100的二维数组

#### 标准化

将所有特征的尺度都缩放在 -1到1之间，这样可以帮助梯度下降算法更快的收敛

## 三、实验结果及分析

### 1. 实验结果展示示例（可图可表可文字，尽量可视化）

<img src="D:\CodeField\TP\image-20220522233249645.png" alt="image-20220522233249645" style="zoom:50%;" />

#### 学习率为0.0000001，迭代次数为5000

<img src="D:\CodeField\TP\image-20220522233434054.png" alt="image-20220522233434054" style="zoom:50%;" />

#### 学习率为0.001，迭代次数为5000

<img src="D:\CodeField\TP\image-20220522233633233.png" alt="image-20220522233633233" style="zoom:50%;" />

#### 学习率为0.1，迭代次数为5000

<img src="D:\CodeField\TP\image-20220522233751523.png" alt="image-20220522233751523" style="zoom:50%;" />

#### 学习率为0.1，迭代次数为500

<img src="D:\CodeField\TP\image-20220522233955810.png" alt="image-20220522233955810" style="zoom:50%;" />

#### 学习率为0.1，迭代次数为50000

<img src="D:\CodeField\TP\image-20220522234128953.png" alt="image-20220522234128953" style="zoom:50%;" />

#### 学习率为0.000000001，迭代次数为500000

<img src="D:\CodeField\TP\image-20220522234455861.png" alt="image-20220522234455861" style="zoom:50%;" />

### 2.评测指标展示及分析（其它可分析运行时间等）

​	从上面的loss图可以看出，当迭代次数为5000时，学习率越低，loss降得越慢，学习率越高，loss下降得快，但是用下面的方法测出来的准确度反而降低了

<img src="D:\CodeField\TP\image-20220522233836936.png" alt="image-20220522233836936" style="zoom:50%;" />

## 四、参考资料

理论课和实验课PPT

[逻辑回归实战练习——根据学生成绩预测是否被录取 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/61414743)
